<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AFO AI Feed Digest</title>
  <link href="https://github.com/tenki/afo" rel="alternate"/>
  <link href="https://github.com/tenki/afo" rel="self"/>
  <id>https://github.com/tenki/afo</id>
  <updated>2026-01-03T14:50:10.486Z</updated>
  <subtitle>Automatic summaries generated from Feeds.opml sources.</subtitle>
  <entry>
    <title>How To Design For (And With) Deaf People</title>
    <link href="https://smashingmagazine.com/2025/12/how-design-for-with-deaf-people/" rel="alternate"/>
    <id>https://smashingmagazine.com/2025/12/how-design-for-with-deaf-people/</id>
    <updated>2025-12-30T10:00:00.000Z</updated>
    <published>2025-12-30T10:00:00.000Z</published>
    <author>
      <name>Articles on Smashing Magazine — For Web Designers And Developers</name>
    </author>
    <content type="html">Inclusive design for deaf users requires multiple communication channels, visual alternatives, proper terminology, and direct community engagement, recognizing deaf culture&apos;s diversity.&lt;br/&gt;&lt;br/&gt;------&lt;br/&gt;&lt;br/&gt;This article outlines critical considerations for inclusive design when working with deaf and hard-of-hearing users, emphasizing the diversity within the deaf community—from varying hearing loss levels (16-25 dB to 91+ dB) to the fact that only 1% of US deaf people know sign language, with no universal sign language among the 300+ variants globally. Key design principles include providing multiple communication channels (never phone-only), comprehensive visual alternatives (captions, transcripts with speaker IDs and sound descriptions), and haptic feedback, while recognizing that written communication, though often effective, shouldn&apos;t be assumed accessible to all since spoken/written language is frequently a second language for sign language users. The article stresses proper terminology (capital-D &quot;Deaf&quot; for cultural identity, lowercase &quot;deaf&quot; for late-deafened individuals, avoiding &quot;impairment&quot;), understanding that lip-reading captures only 30% of words, and recognizing sign languages as complex 4D spatial languages with unique grammar. Most importantly, designers must directly engage the deaf community in testing and co-creation rather than making assumptions, designing *with* deaf people rather than just *for* them.&lt;br/&gt;&lt;br/&gt;------&lt;br/&gt;&lt;br/&gt;Key sections:&lt;br/&gt;[1] Deafness Is Spectrum: Deafness ranges from slight (16-25 dB) to profound hearing loss (91+ dB), with each level affecting speech comprehension differently. Around 90-95% of deaf people come from hearing families, and hearing loss often occurs due to loud noises, age, disease, or accidents rather than being congenital.&lt;br/&gt;&lt;br/&gt;[2] Sign Language Facts: Only about 1% of deaf people in the US know sign language, and there is no universal sign language—there are around 300 different sign languages globally. Sign languages are 4D spatial languages with unique grammar and syntax, incorporating 3D space, time, and facial expressions, and they don&apos;t have a written form.&lt;br/&gt;&lt;br/&gt;[3] Communication Best Practices: Written communication is often best since spoken language is frequently a second language for deaf people, but don&apos;t assume all deaf people are comfortable with written text. Only about 30% of words can be understood through lip-reading, making additional visual cues essential.&lt;br/&gt;&lt;br/&gt;[4] Respectful Terminology Usage: Use &apos;Deaf&apos; (capital D) for culturally Deaf people who use sign language as their first language, &apos;deaf&apos; (lowercase) for those who became deaf later in life, and &apos;Hard of Hearing&apos; for mild to moderate hearing loss. Always ask individuals their preference and avoid terms like &apos;hearing impairment.&apos;&lt;br/&gt;&lt;br/&gt;[5] UX Design Guidelines: Never make phone calls the only contact method, and always provide text alternatives for audible alerts and add haptic feedback on mobile. Include transcripts, closed captions, speaker identification, and descriptions of non-spoken sounds for all audio and video content.&lt;br/&gt;&lt;br/&gt;[6] Inclusive Design Principles: Design multiple communication methods for every interaction, ensure good lighting for facial expression visibility, and use circular seating arrangements when possible. Always test products with the actual deaf community rather than making assumptions, designing with people instead of for them.</content>
  </entry>
  <entry>
    <title>Giving Users A Voice Through Virtual Personas</title>
    <link href="https://smashingmagazine.com/2025/12/giving-users-voice-virtual-personas/" rel="alternate"/>
    <id>https://smashingmagazine.com/2025/12/giving-users-voice-virtual-personas/</id>
    <updated>2025-12-23T10:00:00.000Z</updated>
    <published>2025-12-23T10:00:00.000Z</published>
    <author>
      <name>Articles on Smashing Magazine — For Web Designers And Developers</name>
    </author>
    <content type="html">AI-powered virtual personas transform static user research into interactive, queryable systems that provide real-time, multi-perspective feedback to organizational stakeholders.&lt;br/&gt;&lt;br/&gt;------&lt;br/&gt;&lt;br/&gt;# Summary&lt;br/&gt;&lt;br/&gt;This article explores using AI to make static user research personas interactive and accessible across organizations. Rather than letting research languish in shared drives, AI systems can synthesize insights from centralized data sources (surveys, interviews, analytics, support tickets) and respond to stakeholder queries with multi-perspective feedback in real-time. AI-enhanced personas can be significantly more detailed than traditional versions, incorporating contradictory data, nuanced context, and role-specific lenses that adapt responses based on who&apos;s asking. Implementation ranges from simple solutions using ChatGPT Projects or Claude workspaces with uploaded documents, to more sophisticated approaches (not fully detailed in the excerpt). The core value proposition is transforming user research from static documentation into an actionable, queryable system that enables UX-informed decision-making at the moment of need.&lt;br/&gt;&lt;br/&gt;------&lt;br/&gt;&lt;br/&gt;Key sections:&lt;br/&gt;[1] Research Distribution Problem: Organizations conduct user research and create personas, but this valuable information sits unused in shared drives and repositories. Teams across the organization make UX-impacting decisions without consulting research because it&apos;s difficult to access and interpret at the moment of need.&lt;br/&gt;&lt;br/&gt;[2] AI-Powered Interactive Solution: AI can transform static research into an interactive system where stakeholders ask questions and receive synthesized insights across all user personas. Instead of searching through documents, teams get consolidated multi-perspective feedback from a single query, making research actionable in real-time.&lt;br/&gt;&lt;br/&gt;[3] Building Research Repository: The foundation is centralizing scattered user research data from surveys, interviews, support tickets, analytics, and existing personas into a single source of truth. AI can process messy, unorganized inputs, and deep research tools like Perplexity can establish baselines when primary research is limited.&lt;br/&gt;&lt;br/&gt;[4] Enhanced Persona Creation: Unlike traditional scannable personas, AI-consumed personas can be highly detailed with lengthy observations, contradictory data, and nuanced context. These personas can include different lenses (marketing, product, support) tailored to specific business functions, with AI drawing relevant information based on who asks the question.&lt;br/&gt;&lt;br/&gt;[5] Implementation Approaches: Solutions range from simple to sophisticated: basic setup uses AI platform features like ChatGPT Projects or Claude workspaces to upload research documents and personas with clear instructions. The article indicates more advanced options exist but the content appears truncated before detailing them fully.</content>
  </entry>
  <entry>
    <title>How To Measure The Impact Of Features</title>
    <link href="https://smashingmagazine.com/2025/12/how-measure-impact-features-tars/" rel="alternate"/>
    <id>https://smashingmagazine.com/2025/12/how-measure-impact-features-tars/</id>
    <updated>2025-12-19T10:00:00.000Z</updated>
    <published>2025-12-19T10:00:00.000Z</published>
    <author>
      <name>Articles on Smashing Magazine — For Web Designers And Developers</name>
    </author>
    <content type="html">TARS framework measures feature impact using Target, Adoption, Retention, and Satisfaction metrics to quantify problem-solution fit and guide product decisions.&lt;br/&gt;&lt;br/&gt;------&lt;br/&gt;&lt;br/&gt;# Summary&lt;br/&gt;&lt;br/&gt;TARS is a UX metric framework created by Adrian H. Raudschl that measures feature impact through four key metrics: **Target** (percentage of users with the problem), **Adoption** (users engaging with the solution), **Retention** (continued usage over time), and **Satisfaction** (ease of problem-solving). The framework uses an S÷T score (Satisfied Users ÷ Target Users) to visualize features across a 2×2 matrix, helping identify strategic value by categorizing them as overperforming, liability, core, or project features. Unlike conversion rate—which the framework considers unreliable for UX measurement due to external business factors—TARS focuses on problem-solution fit and actual user behavior patterns, with benchmarks like &amp;gt;60% adoption indicating strong impact and &amp;gt;50% retention signaling high strategic importance. This approach provides product teams with a repeatable, quantifiable method to demonstrate UX work effectiveness and make data-driven decisions about feature investment.&lt;br/&gt;&lt;br/&gt;------&lt;br/&gt;&lt;br/&gt;Key sections:&lt;br/&gt;[1] TARS Framework Introduction: TARS is a simple, repeatable UX metric framework designed to track product feature performance. Created by Adrian H. Raudschl, it provides a meaningful way to measure feature impact through business metrics and visualize UX work effectiveness.&lt;br/&gt;&lt;br/&gt;[2] Target Audience Measurement: Quantifies what percentage of product users have the specific problem a feature aims to solve. This differs from feature usage, as more users may have the problem but can&apos;t find the solution.&lt;br/&gt;&lt;br/&gt;[3] Adoption Rate Tracking: Measures how many target users actually engage meaningfully with a feature over time. High adoption (&amp;gt;60%) suggests impactful problem-solving, while low adoption (&amp;lt;20%) may indicate existing workarounds or discoverability issues.&lt;br/&gt;&lt;br/&gt;[4] Retention Analysis: Tracks how many users who initially adopted a feature continue using it repeatedly over time. A &amp;gt;50% retention rate signals high strategic importance, while 25-35% indicates medium significance.&lt;br/&gt;&lt;br/&gt;[5] Satisfaction Score (CES): Measures satisfaction levels among retained users by asking how easy it was to solve their problem after using the feature. This helps identify hidden issues not reflected in retention scores.&lt;br/&gt;&lt;br/&gt;[6] Feature Strategy Matrix: Uses S÷T score (Satisfied Users ÷ Target Users) to map features across a 2×2 quadrants matrix. This visualization helps identify overperforming, liability, core, and project features for strategic decision-making.&lt;br/&gt;&lt;br/&gt;[7] Conversion Rate Limitations: Conversion rate is not a reliable UX metric because high conversion can occur despite poor UX (due to brand power, pricing, or lack of alternatives) and low conversion can happen despite great UX (due to business model, trust issues, or market factors). UX metrics should focus on task completion, time on task, error reduction, and decision paralysis instead.</content>
  </entry>
</feed>
